# -*- coding: utf-8 -*-
"""NLP_Model_with_Tensorflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fEB-aC2vh24YPMg-ZjMDUaYZq9JjlUmN

Nama : Randi Julian Saputra

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/randiijulian/Pengembangan-Machine-Learning-Dicoding/blob/main/NLP%20Model%20With%20Tensorflow/NLP_Model_with_Tensorflow.ipynb)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Profile-blue?logo=linkedin)](https://www.linkedin.com/in/randijulian/)
[![Github](https://img.shields.io/badge/GitHub-Profile-lightgrey?logo=github)](https://github.com/randiijulian)
[![Dataset](https://img.shields.io/badge/Dataset-Download-green)](https://www.kaggle.com/datasets/abdallahwagih/emotion-dataset)
"""

from google.colab import drive
drive.mount('/content/drive')

"""Install Needed Library"""

#Import Library
import pandas as pd #Pandas to process dataset
from sklearn.model_selection import train_test_split #split dataset into training and validation
from tensorflow.keras.preprocessing.text import Tokenizer #make tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences #make sequences
import tensorflow as tf #tensorflow

!pip install -U -q kaggle # install kaggle for using kaggle
!mkdir -p ~/.kaggle
!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/ # use API kaggle for import file from kaggle
# download file from kaggle and place to folder dataset
!kaggle datasets download -d abdallahwagih/emotion-dataset -p "/content/"
!ls

# Unzip file dataset
import zipfile
local_zip = '/content/emotion-dataset.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content/')
zip_ref.close()

#read dataset
home_data = pd.read_csv('/content/Emotion_classify_Data.csv')
home_data.info()
home_data.head()

"""#Exploratory Data Analysis - Menangani Missing Value dan Outliers"""

category = pd.get_dummies(home_data.Emotion)
new_data = pd.concat([home_data, category], axis=1)
new_data = new_data.drop(columns='Emotion')
new_data

text = new_data['Comment'].values
label = new_data[['anger','fear','joy']].values

text

label

from sklearn.model_selection import train_test_split
train_text, test_text, label_train, label_test = train_test_split(text, label, test_size=0.2)

"""# Modeling

## CNN
"""

import numpy as np
label_train=np.array(label_train)
label_test=np.array(label_test)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=1000, oov_token='x')
tokenizer.fit_on_texts(train_text)
tokenizer.fit_on_texts(test_text)

sekuens_latih = tokenizer.texts_to_sequences(train_text)
sekuens_test = tokenizer.texts_to_sequences(test_text)

pad_train=pad_sequences(sekuens_latih,
                        padding='post',
                        maxlen=50,
                        truncating='post')

pad_test=pad_sequences(sekuens_test,
                       padding='post',
                       maxlen=50,
                       truncating='post')
# pad_train = pad_sequences(sekuens_latih, maxlen=100)
# pad_test = pad_sequences(sekuens_test,  maxlen=100)

# #create tokenizer and padding
# tokenizer = Tokenizer(num_words=20000, oov_token='x')
# tokenizer.fit_on_texts(train_text)
# word_index = tokenizer.word_index
# sekuens_train = tokenizer.texts_to_sequences(train_text)
# sekuens_test = tokenizer.texts_to_sequences(test_text)

# padded_train = pad_sequences(sekuens_train,
#                              padding='post',
#                              maxlen=50,
#                              truncating='post')

# padded_test = pad_sequences(sekuens_test,
#                             padding='post',
#                             maxlen=50,
#                             truncating='post')

# import tensorflow as tf
# model = tf.keras.Sequential([
#     tf.keras.layers.Embedding(input_dim=20000,output_dim=24),
#     tf.keras.layers.Dropout(0.2),
#     tf.keras.layers.LSTM(64),
#     tf.keras.layers.Dense(32,activation='relu'),
#     # tf.keras.layers.BatchNormalization(),
#     tf.keras.layers.Dense(64,activation='relu'),
#     tf.keras.layers.Dense(64,activation='relu'),
#     tf.keras.layers.Dense(128,activation='relu'),
#     tf.keras.layers.Dense(128,activation='relu'),
#     # tf.keras.layers.Dropout(0.2),
#     tf.keras.layers.Dense(3, activation='softmax')
# ])

# model.compile(loss='categorical_crossentropy',
#               optimizer='adam',
#               metrics=['accuracy'])

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=20000,output_dim=16),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(64,activation='relu'),
    tf.keras.layers.Dense(128,activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
#     monitor='val_loss',
#     factor=0.002,
#     patience=20,
#     min_lr=1.5e-6
# )

# early_stop = tf.keras.callbacks.EarlyStopping(
#     monitor="val_loss",
#     min_delta=0,
#     patience=12,
#     verbose=0,
#     mode="auto",
#     baseline=None,
#     restore_best_weights=True)

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=5,
    min_lr=1.5e-5
)

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0,
    patience=12,
    verbose=0,
    mode="auto",
    baseline=None,
    restore_best_weights=True)

num_epochs = 100
history = model.fit(pad_train, label_train, epochs=num_epochs, batch_size=32,callbacks=[reduce_lr, early_stop],
                    validation_data=(pad_test, label_test),verbose=1)

# #Callback to avoid overfitting
# class myCallback(tf.keras.callbacks.Callback):
#   def on_epoch_end(self, epoch, logs={}):
#     if(logs.get('accuracy')>0.9 and logs.get('val_accuracy')>0.9):
#       print("\nAkurasi telah lebih dari 90%!")
#       self.model.stop_training = Truecla
#     monitor="val_loss",
#     min_delta=0,
#     patience=12,
#     verbose=0,
#     mode="auto",
#     baseline=None,
#     restore_best_weights=True
# callbacks = myCallback()

# plot_model = model.fit(
#     padded_train,
#     label_train,
#     epochs=100, #total epoch
#     batch_size=32,
#     callbacks=[reduce_lr, callbacks],
#     validation_data=(padded_test, label_test),
#     verbose=1
#     )

import matplotlib.pyplot as plt
#Accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
#Loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()