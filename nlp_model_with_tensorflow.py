# -*- coding: utf-8 -*-
"""NLP_Model_with_Tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/randiijulian/Pengembangan-Machine-Learning-Dicoding/blob/main/NLP_Model_with_Tensorflow.ipynb

Nama : Randi Julian Saputra

Github : github.com/randiijulian

Dataset : kaggle.com/datasets/onpilot/indonesian-tweets-covid19-handling-2020

## Project Submission 1 Dicoding Pengembangan ML
NLP Model
"""

from google.colab import drive
drive.mount('/content/drive')

"""Install Needed Library"""

#Import Library
import pandas as pd #Pandas to process dataset
from sklearn.model_selection import train_test_split #split dataset into training and validation  
from tensorflow.keras.preprocessing.text import Tokenizer #make tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences #make sequences
import tensorflow as tf #tensorflow

#install kaggle
!pip install -U -q kaggle
!mkdir -p ~/.kaggle

#input file ke colab
from google.colab import files 
files.upload()

#.json untuk API kaggle
!cp kaggle.json ~/.kaggle/

#download dataset menggunakan link kaggle
!kaggle datasets download -d onpilot/indonesian-tweets-covid19-handling-2020
!ls

# Unzip file dataset
import zipfile
local_zip = '/content/indonesian-tweets-covid19-handling-2020.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content')
zip_ref.close()

#read dataset
home_data = pd.read_excel('/content/prastyo_sentiment_covid-general_aspect.xlsx')
home_data.head()

home_data.info()

# Deskripsi
home_data.describe()

"""#Exploratory Data Analysis - Menangani Missing Value dan Outliers"""

home_data = home_data.drop(['Total Data', 'Unnamed: 2', '=', 2269],axis=1)

home_data

# #drop category and split category
# kategori_berita = pd.get_dummies(home_data.category)
# dataset_baru = pd.concat([home_data, kategori_berita], axis=1)
# dataset_baru = dataset_baru.drop(columns='category')
# dataset_baru

sentiment = pd.get_dummies(home_data.sentiment)
sentiment

new_data = pd.concat([home_data, sentiment], axis=1)
new_data = new_data.drop(columns='sentiment')
new_data

#preparing text and label
text = new_data['text'].values
label = new_data[['Negative', 'Neutral', 'Positive']].values

#Divide the dataset into train and validation
train_text, test_text, label_train, label_test = train_test_split(text, label, test_size=0.2)

import numpy as np
label_train=np.array(label_train)
label_test=np.array(label_test)

#create tokenizer and padding
tokenizer = Tokenizer(num_words=2000, oov_token='x')
tokenizer.fit_on_texts(train_text)
word_index = tokenizer.word_index
sekuens_train = tokenizer.texts_to_sequences(train_text)
sekuens_test = tokenizer.texts_to_sequences(test_text)
padded_train = pad_sequences(sekuens_train) 
padded_test = pad_sequences(sekuens_test)

#Callback to avoid overfitting
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9 and logs.get('val_accuracy')>0.9):
      print("\nAkurasi telah lebih dari 90%!")
      self.model.stop_training = Truecla
callbacks = myCallback()

#create model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax') #total layer/class pada label
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

plot_model = model.fit(
    padded_train, 
    label_train, 
    epochs=30, #total epoch 
    validation_data=(padded_test, label_test), 
    verbose=2, 
    callbacks=[callbacks]
    )

import matplotlib.pyplot as plt
#Accuracy  
plt.plot(plot_model.history['accuracy'])
plt.plot(plot_model.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
#Loss
plt.plot(plot_model.history['loss'])
plt.plot(plot_model.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

